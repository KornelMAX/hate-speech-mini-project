model,tn,fp,fn,tp,fp_rate,fn_rate,precision_nonhate,recall_nonhate,f1_nonhate,precision_hate,recall_hate,f1_hate,brier_score
svm,4430,28,127,210,0.006280843427545985,0.3768545994065282,0.9721307877989905,0.993719156572454,0.9828064337215752,0.8823529411764706,0.6231454005934718,0.7304347826086955,0.026585566666494875
bertweet_off_the_shelf,3589,869,147,190,0.1949304620906236,0.4362017804154303,0.9606531049250535,0.8050695379093764,0.8760068342689773,0.1794145420207743,0.5637982195845698,0.2722063037249284,0.19077300996531918
bertweet_finetuned_weighted,4382,76,60,277,0.01704800358905339,0.17804154302670624,0.9864925709140027,0.9829519964109467,0.9847191011235954,0.7847025495750708,0.8219584569732937,0.8028985507246377,0.026091209062425402
tw_roberta_hate_off_the_shelf,4351,107,281,56,0.024001794526693584,0.8338278931750742,0.9393350604490501,0.9759982054733064,0.9573157315731574,0.34355828220858897,0.1661721068249258,0.224,0.07514448725834333
tw_roberta_hate_finetuned_weighted,4418,40,56,281,0.008972633467922835,0.1661721068249258,0.9874832364774251,0.9910273665320771,0.9892521271831616,0.8753894080996885,0.8338278931750742,0.8541033434650457,0.01776214716817602
